model:
  vocab_size: 10000
  d_model: 512
  nhead: 8
  num_layers: 6
  dim_feedforward: 2048
  dropout: 0.1

training:
  lr: 0.0005
  epochs: 10
  batch_size: 32
  log_interval: 100
  checkpoint_dir: "../../models/trained"

data:
  train_file: "../../data/processed/train.txt"
