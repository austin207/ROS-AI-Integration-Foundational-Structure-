# Global configuration for the project

model:
  vocab_size: 10000

llm_training:
  d_model: 512
  nhead: 8
  num_layers: 6
  dim_feedforward: 2048
  dropout: 0.1
  lr: 0.0005
  epochs: 10
  batch_size: 32
  log_interval: 100
  checkpoint_dir: "models/trained"
  train_file: "data/processed/train.txt"

distillation:
  teacher:
    d_model: 512
    nhead: 8
    num_layers: 6
    dim_feedforward: 2048
    dropout: 0.1
  student:
    d_model: 256
    nhead: 4
    num_layers: 4
    dim_feedforward: 1024
    dropout: 0.1
  lr: 0.0005
  epochs: 5
  batch_size: 32
  log_interval: 50
  steps_per_epoch: 100
  checkpoint_dir: "models/distilled"
  train_file: "data/processed/train.txt"

ros_integration:
  checkpoint: "models/distilled/checkpoint_epoch_1.pt"
