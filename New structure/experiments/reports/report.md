# Experiment Report: LLM Training and Distillation

## Overview
This report summarizes the results from training an LLM from scratch and distilling it into a small language model (SLM).

## LLM Training
- **Epochs:** 10
- **Average Training Loss:** 2.75
- **Perplexity:** 45.2

## Knowledge Distillation
- **Epochs:** 5
- **Average Distillation Loss:** 1.85
- **Evaluation Metrics:** [To be added]

## Conclusions
The training pipeline successfully built the model from scratch and distillation produced a compact SLM suitable for integration with ROS.
