model:
  vocab_size: 10000

teacher:
  d_model: 512
  nhead: 8
  num_layers: 6
  dim_feedforward: 2048
  dropout: 0.1

student:
  d_model: 256
  nhead: 4
  num_layers: 4
  dim_feedforward: 1024
  dropout: 0.1

distillation:
  lr: 0.0005
  epochs: 5
  batch_size: 32
  log_interval: 50
  steps_per_epoch: 100
  checkpoint_dir: "../../models/distilled"

data:
  train_file: "../../data/processed/train.txt"
